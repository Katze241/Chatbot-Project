# unified_pipeline.py
import os
import time
import numpy as np
from typing import List, Dict, Set
import re
import hashlib
import shutil
import glob
import pickle
import chromadb

# LangChain ë° ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_community.llms import Ollama
from langchain.retrievers import MultiQueryRetriever
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate
from langchain_core.documents import Document

# ê¸°íƒ€ ìœ í‹¸ë¦¬í‹° ë¼ì´ë¸ŒëŸ¬ë¦¬
import tiktoken
from sentence_transformers.cross_encoder import CrossEncoder
from konlpy.tag import Komoran

# --- 1. ì´ˆê¸° ì„¤ì • ë° ëª¨ë¸ ë¡œë“œ ---

# ì„ë² ë”© ëª¨ë¸
embedding_model = HuggingFaceEmbeddings(
    model_name="BAAI/bge-m3",
    model_kwargs={'device': 'cpu'},
    encode_kwargs={'normalize_embeddings': True}
)
# ì¬ìˆœìœ„í™” ëª¨ë¸
reranker_model = CrossEncoder('BAAI/bge-reranker-large', device='cpu')

# LLM ì •ì˜ (Ollama ì‚¬ìš©)
llm_cheap = Ollama(model="qwen2:7b")
llm_powerful = Ollama(model="qwen2:7b")

# --- ìƒìˆ˜ ì •ì˜ ---
DB_NAME = "hybrid_vector_db"
CHROMA_DB_PATH = f"./{DB_NAME}"
LARGE_CHUNK_PICKLE = f"{CHROMA_DB_PATH}_large_chunks.pkl"
L_MAX = 4096
CHUNK_OVERLAP = 256

import jpype

jvm_path = r"C:\Program Files\Java\jdk-11\bin\server\jvm.dll"
jar_path = r"C:\Users\kjmkj\workspace\project004\.venv\Lib\site-packages\konlpy\java\*"

if not jpype.isJVMStarted():
    jpype.startJVM(jvm_path, f"-Djava.class.path={jar_path}", "-Xmx4g")

def count_tokens(text: str) -> int:
    encoding = tiktoken.get_encoding("cl100k_base")
    return len(encoding.encode(text))

def extract_keywords_from_text(text: str, max_keywords: int = 10) -> List[str]:
    words = re.findall(r'[ê°€-í£a-zA-Z0-9]+', text.lower())
    word_freq = {}
    for word in words:
        if len(word) > 1:
            word_freq[word] = word_freq.get(word, 0) + 1
    sorted_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)
    return [word for word, freq in sorted_words[:max_keywords]]

def extract_proper_nouns(text: str, max_keywords: int = 5) -> list[str]:
    komoran = Komoran()
    tagged = komoran.pos(text)
    proper_nouns = [word for word, pos in tagged if pos == 'NNP']
    proper_nouns = [str(word) for word in proper_nouns]
    return list(dict.fromkeys(proper_nouns))[:max_keywords]

# --- ê³µí†µ í•¨ìˆ˜ë“¤ ---
def rerank_documents(query: str, retrieved_docs: List[Document], reranker_model, rerank_n: int = 2) -> List[Document]:
    if not retrieved_docs:
        return []
    pairs = [(query, doc.page_content) for doc in retrieved_docs]
    scores = reranker_model.predict(pairs)
    doc_score_pairs = list(zip(retrieved_docs, scores))
    doc_score_pairs.sort(key=lambda x: x[1], reverse=True)
    return [doc for doc, score in doc_score_pairs[:rerank_n]]

def generate_final_answer_simple(context: str, query: str) -> str:
    """ë‹¨ìˆœ íŒŒì´í”„ë¼ì¸ìš© ë‹µë³€ ìƒì„± í•¨ìˆ˜"""
    if not context.strip():
        prompt = PromptTemplate(
            input_variables=["question"],
            template="""
            ë¬´ì¡°ê±´ ë‹µë³€ì€ "í•œêµ­ì–´"ë¡œ í•´ì£¼ì„¸ìš”.
           [ê´€ë ¨ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤. ì¢€ ë” êµ¬ì²´ì ìœ¼ë¡œ ì§ˆë¬¸í•´ì£¼ì„¸ìš”.]ë¼ê³  ì¶œë ¥ë§Œ í•´ì£¼ì„¸ìš”.

[ì‚¬ìš©ì ì§ˆë¬¸]
{question}

[ë‹µë³€]"""
        )
        chain = LLMChain(llm=llm_cheap, prompt=prompt)
        result = chain.invoke({"question": query})
        return result['text']
    else:
        prompt = PromptTemplate(
            input_variables=["context", "question"],
            template=""" 
            ë‹¹ì‹ ì€ ë¬¸ì„œ ê¸°ë°˜ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ë°˜ë“œì‹œ "í•œêµ­ì–´"ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”.
            
            ì§ˆë¬¸ì— ëŒ€í•œ ì •ë‹µë§Œ ì¶œë ¥í•˜ì„¸ìš”.
            ì„¤ëª…, ì´ìœ , ë¶ˆí•„ìš”í•œ ë¬¸ì¥ì€ ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.
            ì¶œë ¥ì—ëŠ” ì •ë‹µì„ ëª…í™•íˆ ëª…ì‹œí•œ ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.

[ë¬¸ì„œ ë‚´ìš©]
{context}

[ì‚¬ìš©ì ì§ˆë¬¸]
{question}

[ë‹µë³€]"""
        )
        chain = LLMChain(llm=llm_powerful, prompt=prompt)
        result = chain.invoke({"context": context, "question": query})
        return result['text']

def generate_final_answer_complex(context: str, query: str) -> str:
    """ë³µì¡ íŒŒì´í”„ë¼ì¸ìš© ë‹µë³€ ìƒì„± í•¨ìˆ˜"""
    if not context.strip():
        prompt = PromptTemplate(
            input_variables=["question"],
            template="""
            ë¬´ì¡°ê±´ ë‹µë³€ì€ "í•œêµ­ì–´"ë¡œ í•´ì£¼ì„¸ìš”.
           [ê´€ë ¨ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤. ì¢€ ë” êµ¬ì²´ì ìœ¼ë¡œ ì§ˆë¬¸í•´ì£¼ì„¸ìš”.]ë¼ê³  ì¶œë ¥ë§Œ í•´ì£¼ì„¸ìš”.

[ì‚¬ìš©ì ì§ˆë¬¸]
{question}

[ë‹µë³€]"""
        )
        chain = LLMChain(llm=llm_cheap, prompt=prompt)
        result = chain.invoke({"question": query})
        return result['text']
    else:
        prompt = PromptTemplate(
            input_variables=["context", "question"],
            template=""" 
            ë‹¹ì‹ ì€ ë¬¸ì„œ ê¸°ë°˜ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ë°˜ë“œì‹œ "í•œêµ­ì–´"ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”.
            
            ì•„ë˜ ì œê³µëœ ë¬¸ì„œ ë‚´ìš©(Context)ì—ì„œ ì‚¬ìš©ì ì§ˆë¬¸(Question)ì— ëŒ€í•´ ë‹µë³€í•˜ì„¸ìš”.
            - ë°˜ë“œì‹œ ë‹µë³€ ì‹œì‘ì— "[ë¬¸ì„œ ê¸°ë°˜ ë‹µë³€ì…ë‹ˆë‹¤.]"ë¼ëŠ” ë¬¸êµ¬ë¥¼ í¬í•¨í•˜ì„¸ìš”.
            - ë‹µë³€ ë§ˆì§€ë§‰ì— [íŒŒì¼ëª…, í˜ì´ì§€ë²ˆí˜¸] í˜•ì‹ìœ¼ë¡œ ì¶œì²˜ë¥¼ ëª…ì‹œí•˜ì„¸ìš”.
            - ë§Œì•½ contextê°€ ë¹„ì–´ ìˆê±°ë‚˜, [ê´€ë ¨ ë‚´ìš© ì—†ìŒ]ì´ë¼ë©´, "[ë¬¸ì„œ ê¸°ë°˜ ë‹µë³€ì…ë‹ˆë‹¤, ê´€ë ¨ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤.]"ë¼ê³  ì¶œë ¥í•´ì£¼ì„¸ìš”.

[ë¬¸ì„œ ë‚´ìš©]
{context}

[ì‚¬ìš©ì ì§ˆë¬¸]
{question}

[ë‹µë³€]"""
        )
        chain = LLMChain(llm=llm_powerful, prompt=prompt)
        result = chain.invoke({"context": context, "question": query})
        return result['text']

def compress_document_context(query: str, context: str, llm_cheap) -> str:
    """ë³µì¡ íŒŒì´í”„ë¼ì¸ìš© ë¬¸ì„œ ì••ì¶• í•¨ìˆ˜"""
    prompt = PromptTemplate(
        input_variables=["question", "context"],
        template="""
        You are a helpful AI assistant that ALWAYS responds in Korean. ë‹¹ì‹ ì€ í•­ìƒ í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
        [ì—­í• ]
    ë‹¹ì‹ ì€ ë¬¸ë§¥(Context)ì—ì„œ íŠ¹ì • ì§ˆë¬¸(Question)ì— ëŒ€í•œ ë‹µë³€ì˜ ê·¼ê±°ê°€ ë˜ëŠ” ë¬¸ì¥ì„ **ì›ë¬¸ ê·¸ëŒ€ë¡œ** ì°¾ì•„ë‚´ëŠ” ê³ ì •ë°€ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì—”ì§„ì…ë‹ˆë‹¤. ë‹¹ì‹ ì€ ì•„ë˜ [ì˜ˆì‹œ]ì™€ [ì ˆëŒ€ ê·œì¹™]ì— ë”°ë¼ [ì‹¤ì œ ì‘ì—…]ì„ ìˆ˜í–‰í•´ì•¼ í•©ë‹ˆë‹¤.

    [ì ˆëŒ€ ê·œì¹™]
    1.  **ìš”ì•½ ê¸ˆì§€:** ì ˆëŒ€ ë¬¸ì¥ì„ ìš”ì•½í•˜ê±°ë‚˜ ìì‹ ì˜ ì–¸ì–´ë¡œ ì¬ì‘ì„±í•˜ì§€ ë§ˆì„¸ìš”.
    2.  **ë³€ê²½ ê¸ˆì§€:** ë‹¨ì–´ í•˜ë‚˜ë„ ì¶”ê°€í•˜ê±°ë‚˜ ë¹¼ê±°ë‚˜ ë°”ê¾¸ì§€ ë§ˆì„¸ìš”.
    3.  **ì˜ê²¬ ê¸ˆì§€:** ë‹¹ì‹ ì˜ ì˜ê²¬, ë¶€ì—° ì„¤ëª…, ì¸ì‚¬ë§ ë“± ë‹¤ë¥¸ ì–´ë–¤ í…ìŠ¤íŠ¸ë„ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.
    4.  **ì •ë³´ ë¶€ì¬ ì‹œ:** ë§Œì•½ [ë¬¸ë§¥]ì— [ì§ˆë¬¸]ì— ë‹µí•  ì •ë³´ê°€ ì „í˜€ ì—†ë‹¤ë©´, ë‹¤ë¥¸ ë§ ì—†ì´ ì •í™•íˆ **[ê´€ë ¨ ë‚´ìš© ì—†ìŒ]** ì´ë¼ê³ ë§Œ ì¶œë ¥í•´ì•¼ í•©ë‹ˆë‹¤.
    ---

    [ì‹¤ì œ ì‘ì—…]
    [ë¬¸ë§¥]
    {context}

    [ì§ˆë¬¸]
    {question}

    [ì¶”ì¶œëœ ë¬¸ì¥]
        """
    )
    chain = LLMChain(llm=llm_cheap, prompt=prompt)
    result = chain.invoke({"context": context, "question": query})
    return result['text']

# --- íŒŒì´í”„ë¼ì¸ í•¨ìˆ˜ë“¤ ---
def simple_pipeline(
    query: str,
    retrieved_small_docs: List[Document], 
    reranker_model,
    llm_powerful,
    top_k: int = 10,
    rerank_n: int = 3
) -> str:
    """ë‹¨ìˆœ íŒŒì´í”„ë¼ì¸ (small_non_compression.py ê¸°ë°˜)"""
    print(f"[1/3] 1ì°¨ ê²€ìƒ‰ ì™„ë£Œ (í†µí•© ê²€ìƒ‰ëœ small chunk ìˆ˜: {len(retrieved_small_docs)})")

    keywords = extract_proper_nouns(query, max_keywords=5)
    print(f"[ê³ ìœ ëª…ì‚¬ ì¶”ì¶œ] ì¿¼ë¦¬ì—ì„œ ì¶”ì¶œëœ ê³ ìœ ëª…ì‚¬: {keywords}")

    keyword_weight = 0.5
    scored_docs = []
    for doc in retrieved_small_docs:
        num_keywords = sum(1 for kw in keywords if kw in doc.page_content)
        score = 1.0 + (num_keywords * keyword_weight)
        scored_docs.append((doc, score))
    scored_docs.sort(key=lambda x: x[1], reverse=True)
    rerank_candidates = [doc for doc, _ in scored_docs[:top_k]]
    print(f"[í‚¤ì›Œë“œ ì ìˆ˜ ê¸°ë°˜ ì •ë ¬] ìƒìœ„ {len(rerank_candidates)}ê°œ ë¬¸ì„œ ì„ íƒ")

    if not rerank_candidates:
        print("[ì˜ˆì™¸ ì²˜ë¦¬] í‚¤ì›Œë“œ ê´€ë ¨ë„ê°€ ë†’ì€ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. ê´€ë ¨ ë‚´ìš© ì—†ìŒ ë°˜í™˜.")
        return generate_final_answer_simple("", query)

    print("[2/3] ì¬ìˆœìœ„í™”(CrossEncoder) ì§„í–‰ ì¤‘...")
    reranked_small_docs = rerank_documents(query, rerank_candidates, reranker_model, rerank_n)
    print(f"[2/3] ì¬ìˆœìœ„í™” ì™„ë£Œ (ì„ íƒëœ small chunk ìˆ˜: {len(reranked_small_docs)})")

    context = "\n---\n".join([doc.page_content for doc in reranked_small_docs])
    print(f"[Context ìƒì„±] ì¬ìˆœìœ„í™”ëœ {len(reranked_small_docs)}ê°œì˜ chunkë¡œ context êµ¬ì„±")

    print("[3/3] ìµœì¢… ë‹µë³€ ìƒì„± ì¤‘...")
    answer = generate_final_answer_simple(context, query)
    print("[3/3] ìµœì¢… ë‹µë³€ ìƒì„± ì™„ë£Œ!")
    return answer

def complex_pipeline(
    query: str,
    retrieved_small_docs: List[Document], 
    large_chunk_dict: Dict[str, Document],
    reranker_model,
    llm_cheap,
    llm_powerful,
    top_k: int = 10,
    rerank_n: int = 3
) -> str:
    """ë³µì¡ íŒŒì´í”„ë¼ì¸ (online_pipeline_collection.py ê¸°ë°˜)"""
    print(f"[1/5] 1ì°¨ ê²€ìƒ‰ ì™„ë£Œ (í†µí•© ê²€ìƒ‰ëœ small chunk ìˆ˜: {len(retrieved_small_docs)})")

    keywords = extract_proper_nouns(query, max_keywords=5)
    print(f"[ê³ ìœ ëª…ì‚¬ ì¶”ì¶œ] ì¿¼ë¦¬ì—ì„œ ì¶”ì¶œëœ ê³ ìœ ëª…ì‚¬: {keywords}")

    keyword_weight = 0.5
    scored_docs = []
    for doc in retrieved_small_docs:
        num_keywords = sum(1 for kw in keywords if kw in doc.page_content)
        score = 1.0 + (num_keywords * keyword_weight)
        scored_docs.append((doc, score))
    scored_docs.sort(key=lambda x: x[1], reverse=True)
    rerank_candidates = [doc for doc, _ in scored_docs[:top_k]]
    print(f"[í‚¤ì›Œë“œ ì ìˆ˜ ê¸°ë°˜ ì •ë ¬] ìƒìœ„ {len(rerank_candidates)}ê°œ ë¬¸ì„œ ì„ íƒ")

    if not rerank_candidates:
        print("[ì˜ˆì™¸ ì²˜ë¦¬] í‚¤ì›Œë“œ ê´€ë ¨ë„ê°€ ë†’ì€ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. ê´€ë ¨ ë‚´ìš© ì—†ìŒ ë°˜í™˜.")
        return generate_final_answer_complex("", query)

    print("[2/5] ì¬ìˆœìœ„í™”(CrossEncoder) ì§„í–‰ ì¤‘...")
    reranked_small_docs = rerank_documents(query, rerank_candidates, reranker_model, rerank_n)
    print(f"[2/5] ì¬ìˆœìœ„í™” ì™„ë£Œ (ì„ íƒëœ small chunk ìˆ˜: {len(reranked_small_docs)})")

    print("[3/5] parent large chunk ì¶”ì¶œ ì¤‘...")
    parent_large_ids = {doc.metadata["parent_large_id"] for doc in reranked_small_docs if "parent_large_id" in doc.metadata}
    print(f"[3/5] parent large chunk ì¶”ì¶œ ì™„ë£Œ (ëŒ€ìƒ large chunk ìˆ˜: {len(parent_large_ids)})")

    print("[4/5] context ì••ì¶•(í•µì‹¬ ë¬¸ì¥ ì¶”ì¶œ) ì§„í–‰ ì¤‘...")
    compressed_contexts = []
    for idx, large_id in enumerate(parent_large_ids):
        if large_id in large_chunk_dict:
            print(f"      - [{idx+1}/{len(parent_large_ids)}] large chunk({large_id}) ì••ì¶• ì¤‘...")
            large_doc = large_chunk_dict[large_id]
            compressed = compress_document_context(query, large_doc.page_content, llm_cheap)
            if compressed.strip() and "ê´€ë ¨ ë‚´ìš© ì—†ìŒ" not in compressed:
                compressed_contexts.append(compressed)
                print(f"        [ì••ì¶• ê²°ê³¼]\n{compressed}\n")
            else:
                print(f"        [ì••ì¶• ê²°ê³¼ ì—†ìŒ ë˜ëŠ” ê´€ë ¨ ë‚´ìš© ì—†ìŒ]")
            print(f"      - [{idx+1}/{len(parent_large_ids)}] large chunk({large_id}) ì••ì¶• ì™„ë£Œ")
    context = "\n---\n".join(compressed_contexts)
    print(f"[4/5] context ì••ì¶• ì™„ë£Œ (ì••ì¶•ëœ context ë¸”ë¡ ìˆ˜: {len(compressed_contexts)})")

    print("[5/5] ìµœì¢… ë‹µë³€ ìƒì„± ì¤‘...")
    answer = generate_final_answer_complex(context, query)
    print("[5/5] ìµœì¢… ë‹µë³€ ìƒì„± ì™„ë£Œ!")
    return answer

def print_pipeline_menu():
    """íŒŒì´í”„ë¼ì¸ ì„ íƒ ë©”ë‰´ ì¶œë ¥"""
    print("\n" + "="*60)
    print("RAG íŒŒì´í”„ë¼ì¸ ì„ íƒ")
    print("="*60)
    print("1ï¸. ë‹¨ìˆœ íŒŒì´í”„ë¼ì¸ (ë¹ ë¥¸ ì‘ë‹µ)")
    print("   - 3ë‹¨ê³„ ì²˜ë¦¬")
    print("   - Small chunk ë‚´ìš© ì§ì ‘ ì‚¬ìš©")
    print("   - ë¹ ë¥¸ ì‘ë‹µ ì†ë„")
    print("   - ê¸°ë³¸ì ì¸ ë‹µë³€ í’ˆì§ˆ")
    print()
    print("2ï¸. ë³µì¡ íŒŒì´í”„ë¼ì¸ (ì •í™•í•œ ë‹µë³€)")
    print("   - 5ë‹¨ê³„ ì²˜ë¦¬")
    print("   - Large chunk ì••ì¶• ë° í•µì‹¬ ë¬¸ì¥ ì¶”ì¶œ")
    print("   - ìƒëŒ€ì ìœ¼ë¡œ ëŠë¦° ì‘ë‹µ ì†ë„")
    print("   - ë” ì •í™•í•˜ê³  ì§‘ì¤‘ëœ ë‹µë³€")
    print("="*60)

def get_user_choice() -> int:
    """ì‚¬ìš©ì ì„ íƒ ì…ë ¥ ë°›ê¸°"""
    while True:
        try:
            choice = int(input("\níŒŒì´í”„ë¼ì¸ì„ ì„ íƒí•˜ì„¸ìš” (1 ë˜ëŠ” 2): "))
            if choice in [1, 2]:
                return choice
            else:
                print("1 ë˜ëŠ” 2ë§Œ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        except ValueError:
            print("ìˆ«ìë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")

def get_user_query() -> str:
    """ì‚¬ìš©ì ì§ˆë¬¸ ì…ë ¥ ë°›ê¸°"""
    print("\n" + "-"*50)
    query = input("ğŸ’¬ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”: ")
    return query.strip()

# --- main ì‹¤í–‰ë¶€ ---
if __name__ == "__main__":
    if not os.path.exists(CHROMA_DB_PATH) or not os.path.exists(LARGE_CHUNK_PICKLE):
        print(f"--- ì—ëŸ¬: Chroma DB ë˜ëŠ” large chunk pickle ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: '{CHROMA_DB_PATH}', '{LARGE_CHUNK_PICKLE}' ---")
        print("--- ë¨¼ì € offline_pipeline.pyë¥¼ ì‹¤í–‰í•˜ì—¬ DBë¥¼ êµ¬ì¶•í•˜ì„¸ìš”. ---")
        exit()

    # 1. DB í´ë¼ì´ì–¸íŠ¸ ìƒì„± ë° ëª¨ë“  ì»¬ë ‰ì…˜ ì´ë¦„ ê°€ì ¸ì˜¤ê¸°
    client = chromadb.PersistentClient(path=CHROMA_DB_PATH)
    collection_names = [c.name for c in client.list_collections()]
    
    if not collection_names:
        print(f"--- ì—ëŸ¬: DBì— ì»¬ë ‰ì…˜ì´ ì—†ìŠµë‹ˆë‹¤. offline_pipeline.pyë¥¼ ì‹¤í–‰í•˜ì—¬ DBë¥¼ êµ¬ì¶•í•˜ì„¸ìš”. ---")
        exit()
        
    print("í†µí•© ê²€ìƒ‰ ëŒ€ìƒ ì»¬ë ‰ì…˜ ëª©ë¡:")
    for name in collection_names:
        print(f"  - {name}")

    # 2. íŒŒì´í”„ë¼ì¸ ì„ íƒ
    print_pipeline_menu()
    pipeline_choice = get_user_choice()
    
    # 3. ì§ˆë¬¸ ì…ë ¥
    query = get_user_query()
    
    if not query:
        print("âŒ ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        exit()

    # 4. íŒŒì´í”„ë¼ì¸ë³„ ì‹¤í–‰
    if pipeline_choice == 1:
        print("\n" + "-"*50)
        print("-- [ë‹¨ìˆœ íŒŒì´í”„ë¼ì¸] í•˜ì´ë¸Œë¦¬ë“œ RAG íŒŒì´í”„ë¼ì¸ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
        print(f"   (ì§ˆë¬¸: {query})")
        print("="*50 + "\n")
        
        start_time = time.time()
        
        # ëª¨ë“  ì»¬ë ‰ì…˜ì—ì„œ ê²€ìƒ‰ ìˆ˜í–‰
        print("[1/3] 1ì°¨ ê²€ìƒ‰: ëª¨ë“  ì»¬ë ‰ì…˜ ëŒ€ìƒìœ¼ë¡œ í†µí•© ë²¡í„° ê²€ìƒ‰ ì¤‘...")
        all_retrieved_docs = []
        for name in collection_names:
            vectorstore = Chroma(
                client=client,
                collection_name=name,
                embedding_function=embedding_model
            )
            retrieved = vectorstore.similarity_search(query, k=10)
            all_retrieved_docs.extend(retrieved)
            print(f"  - '{name}' ì»¬ë ‰ì…˜ ê²€ìƒ‰ ì™„ë£Œ (ê²€ìƒ‰ëœ ë¬¸ì„œ: {len(retrieved)}ê°œ)")

        # ì¤‘ë³µ ë¬¸ì„œ ì œê±°
        unique_docs_dict = {doc.page_content: doc for doc in all_retrieved_docs}
        unique_retrieved_docs = list(unique_docs_dict.values())
        print(f"  - ì¤‘ë³µ ì œê±° í›„ ì´ {len(unique_retrieved_docs)}ê°œì˜ í›„ë³´ ë¬¸ì„œ í™•ë³´")
        
        # ë‹¨ìˆœ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        final_answer = simple_pipeline(
            query=query,
            retrieved_small_docs=unique_retrieved_docs,
            reranker_model=reranker_model,
            llm_powerful=llm_powerful,
            top_k=10, 
            rerank_n=3
        )
        
    else:  # pipeline_choice == 2
        print("\n" + "-"*50)
        print("-- [ë³µì¡ íŒŒì´í”„ë¼ì¸] í•˜ì´ë¸Œë¦¬ë“œ RAG íŒŒì´í”„ë¼ì¸ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
        print(f"   (ì§ˆë¬¸: {query})")
        print("="*50 + "\n")
        
        start_time = time.time()
        
        # ëª¨ë“  ì»¬ë ‰ì…˜ì—ì„œ ê²€ìƒ‰ ìˆ˜í–‰
        print("[1/5] 1ì°¨ ê²€ìƒ‰: ëª¨ë“  ì»¬ë ‰ì…˜ ëŒ€ìƒìœ¼ë¡œ í†µí•© ë²¡í„° ê²€ìƒ‰ ì¤‘...")
        all_retrieved_docs = []
        for name in collection_names:
            vectorstore = Chroma(
                client=client,
                collection_name=name,
                embedding_function=embedding_model
            )
            retrieved = vectorstore.similarity_search(query, k=10)
            all_retrieved_docs.extend(retrieved)
            print(f"  - '{name}' ì»¬ë ‰ì…˜ ê²€ìƒ‰ ì™„ë£Œ (ê²€ìƒ‰ëœ ë¬¸ì„œ: {len(retrieved)}ê°œ)")

        # ì¤‘ë³µ ë¬¸ì„œ ì œê±°
        unique_docs_dict = {doc.page_content: doc for doc in all_retrieved_docs}
        unique_retrieved_docs = list(unique_docs_dict.values())
        print(f"  - ì¤‘ë³µ ì œê±° í›„ ì´ {len(unique_retrieved_docs)}ê°œì˜ í›„ë³´ ë¬¸ì„œ í™•ë³´")
        
        # large chunk pickle ë¡œë“œ
        with open(LARGE_CHUNK_PICKLE, 'rb') as f:
            large_chunk_dict: Dict[str, Document] = pickle.load(f)
        
        # ë³µì¡ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        final_answer = complex_pipeline(
            query=query,
            retrieved_small_docs=unique_retrieved_docs,
            large_chunk_dict=large_chunk_dict,
            reranker_model=reranker_model,
            llm_cheap=llm_cheap,
            llm_powerful=llm_powerful,
            top_k=10, 
            rerank_n=3
        )
    
    end_time = time.time()
    
    print("\n" + "-"*50)
    print("-- ìµœì¢… ë‹µë³€ --")
    print(final_answer)
    print("="*50)
    print(f"--- ì´ ì‹¤í–‰ ì‹œê°„: {end_time - start_time:.2f}ì´ˆ---") 
